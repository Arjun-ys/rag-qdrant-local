# RAG with Qdrant (Local Retrieval-Augmented Generation)

## ğŸš€ Overview

This project implements a **local Retrieval-Augmented Generation (RAG) pipeline** using:

* **Sentence Transformers (MiniLM)** â€“ for embeddings
* **Qdrant (Docker)** â€“ as a vector database
* **Ollama (llama3.2)** â€“ as a local LLM for generation

The system allows you to:

1. Store documents semantically in a vector database
2. Retrieve the most relevant documents using vector similarity search
3. Generate answers grounded in the retrieved context

This makes the model **more accurate, explainable, and fact-grounded** compared to vanilla LLM responses.

---

## ğŸ—ï¸ Architecture

```
User Query
     â†“
Embedding Model (MiniLM)
     â†“
Qdrant Vector Search (Docker)
     â†“
Retrieve Relevant Context
     â†“
LLM (Ollama - llama3.2)
     â†“
Final Answer (Grounded in Retrieved Data)
```

---

## ğŸ”§ Tech Stack

* Python
* Docker
* Qdrant (Vector DB)
* Sentence Transformers (`all-MiniLM-L6-v2`)
* Ollama (`llama3.2`)

---

## ğŸ“¦ Setup

### 1ï¸âƒ£ Start Qdrant (Vector Database)

```bash
docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r rag_app/requirements.txt
```

### 3ï¸âƒ£ Install & start Ollama

Download: https://ollama.com/

Then run:

```bash
ollama pull llama3.2
```

---

## â–¶ï¸ Usage

### Add documents & query the system

```python
from rag_app.rag_app import add_document
from rag_app.rag_pipeline import generate_answer

add_document("Endee is a high-performance vector database.")
add_document("Qdrant is a fast open-source vector store with Docker support.")

answer, context = generate_answer("Which database is high performance?")

print("Answer:", answer)
print("\nRetrieved Context:", context)
```

---

## ğŸ“‚ Project Structure

```
rag_app/
â”‚â”€â”€ embedder.py        # Creates embeddings
â”‚â”€â”€ qdrant_client.py   # Handles vector DB operations
â”‚â”€â”€ rag_app.py         # Document storage & retrieval
â”‚â”€â”€ rag_pipeline.py    # Full RAG pipeline (retrieve + generate)
â”‚â”€â”€ requirements.txt   # Dependencies
```

---

## ğŸ¯ Features

* âœ… Semantic search (not keyword matching)
* âœ… Fully local (no external APIs required)
* âœ… Scalable vector storage via Qdrant
* âœ… LLM responses grounded in real stored data
* âœ… Works with Docker

---

## ğŸš€ Future Improvements

* Add FastAPI backend
* Build a Streamlit chat UI
* Support PDF ingestion
* Enable authentication
* Support multiple collections

---

## ğŸ‘¨â€ğŸ’» Author

Arjun YS
AI/ML Enthusiast | RAG | Vector Databases | Docker
